{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3a8847af",
   "metadata": {},
   "source": [
    "# Environment Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2a49b958",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install gym_super_mario_bros\n",
    "#!pip install stable_baselines3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5d765712",
   "metadata": {},
   "outputs": [],
   "source": [
    "# PPO algorythm\n",
    "from stable_baselines3 import PPO\n",
    "# Monitor for logging \n",
    "from stable_baselines3.common.monitor import Monitor\n",
    "# Vec wrappers to vectorize and frame stack\n",
    "from stable_baselines3.common.vec_env import DummyVecEnv, VecFrameStack\n",
    "# Import simpler movements\n",
    "from gym_super_mario_bros.actions import SIMPLE_MOVEMENT\n",
    "# Import to reduce number of movments\n",
    "from nes_py.wrappers import JoypadSpace\n",
    "# To deal with filepaths\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "29d096e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Running Environment_Setup.ipynb to setup custom Mario environment.\n",
    "%run Environment_Setup.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "17f26273",
   "metadata": {},
   "outputs": [],
   "source": [
    "LOG_DIR = './logs/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d4b8b345",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create environment \n",
    "env = Mario()\n",
    "env = JoypadSpace(env, SIMPLE_MOVEMENT)\n",
    "env = Monitor(env, LOG_DIR)\n",
    "env = DummyVecEnv([lambda: env])\n",
    "env = VecFrameStack(env, 4, channels_order='last')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8952bd46",
   "metadata": {},
   "source": [
    "# Setup Callback To Monitor Training Process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7890bdad",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install stable-baselines3[extra]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "54ae09fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import base callback \n",
    "from stable_baselines3.common.callbacks import BaseCallback"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4ec6cfc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TrainAndLoggingCallback(BaseCallback):\n",
    "\n",
    "    def __init__(self, check_freq, save_path, verbose=1):\n",
    "        super(TrainAndLoggingCallback, self).__init__(verbose)\n",
    "        self.check_freq = check_freq\n",
    "        self.save_path = save_path\n",
    "\n",
    "    def _init_callback(self):\n",
    "        if self.save_path is not None:\n",
    "            os.makedirs(self.save_path, exist_ok=True)\n",
    "\n",
    "    def _on_step(self):\n",
    "        if self.n_calls % self.check_freq == 0:\n",
    "            model_path = os.path.join(self.save_path, 'best_model_{}'.format(self.n_calls))\n",
    "            self.model.save(model_path)\n",
    "\n",
    "        return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "02ef7606",
   "metadata": {},
   "outputs": [],
   "source": [
    "CHECKPOINT_DIR = './train/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "68982f2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "callback = TrainAndLoggingCallback(check_freq=100_000, save_path=CHECKPOINT_DIR)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2675ab30",
   "metadata": {},
   "source": [
    "# Continuing Training Best Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2f5a6072",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wrapping the env in a VecTransposeImage.\n",
      "1.2134019828545453e-07\n",
      "6144\n"
     ]
    }
   ],
   "source": [
    "# Best model\n",
    "model = PPO.load(\"train/old_gen/Newestt_Mario_PPO\", env, verbose=1, tensorboard_log=LOG_DIR)\n",
    "\n",
    "# Displaying current learning rate \n",
    "print(model.learning_rate)\n",
    "print(model.n_steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ce9d4c3b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9.707215862836362e-07\n"
     ]
    }
   ],
   "source": [
    "# Lowering learning rate if needed\n",
    "model.learning_rate /= 2\n",
    "print(model.learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "01b02aad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logging to ./logs/PPO_3\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.44e+03 |\n",
      "|    ep_rew_mean     | 226      |\n",
      "| time/              |          |\n",
      "|    fps             | 100      |\n",
      "|    iterations      | 1        |\n",
      "|    time_elapsed    | 61       |\n",
      "|    total_timesteps | 6144     |\n",
      "---------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 1.51e+03     |\n",
      "|    ep_rew_mean          | 363          |\n",
      "| time/                   |              |\n",
      "|    fps                  | 88           |\n",
      "|    iterations           | 2            |\n",
      "|    time_elapsed         | 138          |\n",
      "|    total_timesteps      | 12288        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0003369546 |\n",
      "|    clip_fraction        | 0            |\n",
      "|    clip_range           | 0.187        |\n",
      "|    entropy_loss         | -0.589       |\n",
      "|    explained_variance   | 0.304        |\n",
      "|    learning_rate        | 1.21e-07     |\n",
      "|    loss                 | 433          |\n",
      "|    n_updates            | 4900         |\n",
      "|    policy_gradient_loss | 2.59e-05     |\n",
      "|    value_loss           | 355          |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 1.47e+03     |\n",
      "|    ep_rew_mean          | 293          |\n",
      "| time/                   |              |\n",
      "|    fps                  | 86           |\n",
      "|    iterations           | 3            |\n",
      "|    time_elapsed         | 214          |\n",
      "|    total_timesteps      | 18432        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0005541256 |\n",
      "|    clip_fraction        | 0.000602     |\n",
      "|    clip_range           | 0.187        |\n",
      "|    entropy_loss         | -0.545       |\n",
      "|    explained_variance   | 0.473        |\n",
      "|    learning_rate        | 1.21e-07     |\n",
      "|    loss                 | 88.5         |\n",
      "|    n_updates            | 4910         |\n",
      "|    policy_gradient_loss | 4.1e-06      |\n",
      "|    value_loss           | 213          |\n",
      "------------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 1.54e+03    |\n",
      "|    ep_rew_mean          | 301         |\n",
      "| time/                   |             |\n",
      "|    fps                  | 85          |\n",
      "|    iterations           | 4           |\n",
      "|    time_elapsed         | 288         |\n",
      "|    total_timesteps      | 24576       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.000989057 |\n",
      "|    clip_fraction        | 0.000911    |\n",
      "|    clip_range           | 0.187       |\n",
      "|    entropy_loss         | -0.51       |\n",
      "|    explained_variance   | 0.458       |\n",
      "|    learning_rate        | 1.21e-07    |\n",
      "|    loss                 | 133         |\n",
      "|    n_updates            | 4920        |\n",
      "|    policy_gradient_loss | -0.000636   |\n",
      "|    value_loss           | 132         |\n",
      "-----------------------------------------\n",
      "-------------------------------------------\n",
      "| rollout/                |               |\n",
      "|    ep_len_mean          | 1.64e+03      |\n",
      "|    ep_rew_mean          | 350           |\n",
      "| time/                   |               |\n",
      "|    fps                  | 85            |\n",
      "|    iterations           | 5             |\n",
      "|    time_elapsed         | 358           |\n",
      "|    total_timesteps      | 30720         |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 0.00010194883 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.187         |\n",
      "|    entropy_loss         | -0.463        |\n",
      "|    explained_variance   | 0.507         |\n",
      "|    learning_rate        | 1.21e-07      |\n",
      "|    loss                 | 32.2          |\n",
      "|    n_updates            | 4930          |\n",
      "|    policy_gradient_loss | -6.63e-05     |\n",
      "|    value_loss           | 149           |\n",
      "-------------------------------------------\n",
      "-------------------------------------------\n",
      "| rollout/                |               |\n",
      "|    ep_len_mean          | 1.67e+03      |\n",
      "|    ep_rew_mean          | 351           |\n",
      "| time/                   |               |\n",
      "|    fps                  | 84            |\n",
      "|    iterations           | 6             |\n",
      "|    time_elapsed         | 438           |\n",
      "|    total_timesteps      | 36864         |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 0.00024672385 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.187         |\n",
      "|    entropy_loss         | -0.575        |\n",
      "|    explained_variance   | 0.171         |\n",
      "|    learning_rate        | 1.21e-07      |\n",
      "|    loss                 | 142           |\n",
      "|    n_updates            | 4940          |\n",
      "|    policy_gradient_loss | -0.000155     |\n",
      "|    value_loss           | 834           |\n",
      "-------------------------------------------\n",
      "-------------------------------------------\n",
      "| rollout/                |               |\n",
      "|    ep_len_mean          | 1.72e+03      |\n",
      "|    ep_rew_mean          | 363           |\n",
      "| time/                   |               |\n",
      "|    fps                  | 82            |\n",
      "|    iterations           | 7             |\n",
      "|    time_elapsed         | 521           |\n",
      "|    total_timesteps      | 43008         |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 0.00020911831 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.187         |\n",
      "|    entropy_loss         | -0.604        |\n",
      "|    explained_variance   | 0.198         |\n",
      "|    learning_rate        | 1.21e-07      |\n",
      "|    loss                 | 1.13e+03      |\n",
      "|    n_updates            | 4950          |\n",
      "|    policy_gradient_loss | -0.000122     |\n",
      "|    value_loss           | 822           |\n",
      "-------------------------------------------\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[1;32mIn [15]\u001b[0m, in \u001b[0;36m<cell line: 2>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Training\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlearn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtotal_timesteps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m600_000\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcallback\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcallback\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\RL46\\lib\\site-packages\\stable_baselines3\\ppo\\ppo.py:317\u001b[0m, in \u001b[0;36mPPO.learn\u001b[1;34m(self, total_timesteps, callback, log_interval, eval_env, eval_freq, n_eval_episodes, tb_log_name, eval_log_path, reset_num_timesteps, progress_bar)\u001b[0m\n\u001b[0;32m    303\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mlearn\u001b[39m(\n\u001b[0;32m    304\u001b[0m     \u001b[38;5;28mself\u001b[39m: PPOSelf,\n\u001b[0;32m    305\u001b[0m     total_timesteps: \u001b[38;5;28mint\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    314\u001b[0m     progress_bar: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[0;32m    315\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m PPOSelf:\n\u001b[1;32m--> 317\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlearn\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    318\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtotal_timesteps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtotal_timesteps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    319\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcallback\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcallback\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    320\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlog_interval\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlog_interval\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    321\u001b[0m \u001b[43m        \u001b[49m\u001b[43meval_env\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43meval_env\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    322\u001b[0m \u001b[43m        \u001b[49m\u001b[43meval_freq\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43meval_freq\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    323\u001b[0m \u001b[43m        \u001b[49m\u001b[43mn_eval_episodes\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mn_eval_episodes\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    324\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtb_log_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtb_log_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    325\u001b[0m \u001b[43m        \u001b[49m\u001b[43meval_log_path\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43meval_log_path\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    326\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreset_num_timesteps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreset_num_timesteps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    327\u001b[0m \u001b[43m        \u001b[49m\u001b[43mprogress_bar\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprogress_bar\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    328\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\RL46\\lib\\site-packages\\stable_baselines3\\common\\on_policy_algorithm.py:283\u001b[0m, in \u001b[0;36mOnPolicyAlgorithm.learn\u001b[1;34m(self, total_timesteps, callback, log_interval, eval_env, eval_freq, n_eval_episodes, tb_log_name, eval_log_path, reset_num_timesteps, progress_bar)\u001b[0m\n\u001b[0;32m    280\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlogger\u001b[38;5;241m.\u001b[39mrecord(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtime/total_timesteps\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_timesteps, exclude\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtensorboard\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    281\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlogger\u001b[38;5;241m.\u001b[39mdump(step\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_timesteps)\n\u001b[1;32m--> 283\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    285\u001b[0m callback\u001b[38;5;241m.\u001b[39mon_training_end()\n\u001b[0;32m    287\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\RL46\\lib\\site-packages\\stable_baselines3\\ppo\\ppo.py:214\u001b[0m, in \u001b[0;36mPPO.train\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    211\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39muse_sde:\n\u001b[0;32m    212\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpolicy\u001b[38;5;241m.\u001b[39mreset_noise(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbatch_size)\n\u001b[1;32m--> 214\u001b[0m values, log_prob, entropy \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpolicy\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mevaluate_actions\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrollout_data\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mobservations\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mactions\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    215\u001b[0m values \u001b[38;5;241m=\u001b[39m values\u001b[38;5;241m.\u001b[39mflatten()\n\u001b[0;32m    216\u001b[0m \u001b[38;5;66;03m# Normalize advantage\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\RL46\\lib\\site-packages\\stable_baselines3\\common\\policies.py:645\u001b[0m, in \u001b[0;36mActorCriticPolicy.evaluate_actions\u001b[1;34m(self, obs, actions)\u001b[0m\n\u001b[0;32m    643\u001b[0m latent_pi, latent_vf \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmlp_extractor(features)\n\u001b[0;32m    644\u001b[0m distribution \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_action_dist_from_latent(latent_pi)\n\u001b[1;32m--> 645\u001b[0m log_prob \u001b[38;5;241m=\u001b[39m \u001b[43mdistribution\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlog_prob\u001b[49m\u001b[43m(\u001b[49m\u001b[43mactions\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    646\u001b[0m values \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mvalue_net(latent_vf)\n\u001b[0;32m    647\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m values, log_prob, distribution\u001b[38;5;241m.\u001b[39mentropy()\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\RL46\\lib\\site-packages\\stable_baselines3\\common\\distributions.py:279\u001b[0m, in \u001b[0;36mCategoricalDistribution.log_prob\u001b[1;34m(self, actions)\u001b[0m\n\u001b[0;32m    278\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mlog_prob\u001b[39m(\u001b[38;5;28mself\u001b[39m, actions: th\u001b[38;5;241m.\u001b[39mTensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m th\u001b[38;5;241m.\u001b[39mTensor:\n\u001b[1;32m--> 279\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdistribution\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlog_prob\u001b[49m\u001b[43m(\u001b[49m\u001b[43mactions\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\RL46\\lib\\site-packages\\torch\\distributions\\categorical.py:117\u001b[0m, in \u001b[0;36mCategorical.log_prob\u001b[1;34m(self, value)\u001b[0m\n\u001b[0;32m    115\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mlog_prob\u001b[39m(\u001b[38;5;28mself\u001b[39m, value):\n\u001b[0;32m    116\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_validate_args:\n\u001b[1;32m--> 117\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_validate_sample\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    118\u001b[0m     value \u001b[38;5;241m=\u001b[39m value\u001b[38;5;241m.\u001b[39mlong()\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m    119\u001b[0m     value, log_pmf \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mbroadcast_tensors(value, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlogits)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\RL46\\lib\\site-packages\\torch\\distributions\\distribution.py:286\u001b[0m, in \u001b[0;36mDistribution._validate_sample\u001b[1;34m(self, value)\u001b[0m\n\u001b[0;32m    284\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[0;32m    285\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m support \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m--> 286\u001b[0m valid \u001b[38;5;241m=\u001b[39m \u001b[43msupport\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcheck\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    287\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m valid\u001b[38;5;241m.\u001b[39mall():\n\u001b[0;32m    288\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    289\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mExpected value argument \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    290\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m(\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(value)\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m of shape \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtuple\u001b[39m(value\u001b[38;5;241m.\u001b[39mshape)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m) \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    293\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbut found invalid values:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mvalue\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    294\u001b[0m     )\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\RL46\\lib\\site-packages\\torch\\distributions\\constraints.py:250\u001b[0m, in \u001b[0;36m_IntegerInterval.check\u001b[1;34m(self, value)\u001b[0m\n\u001b[0;32m    249\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcheck\u001b[39m(\u001b[38;5;28mself\u001b[39m, value):\n\u001b[1;32m--> 250\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m (\u001b[43mvalue\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m%\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m) \u001b[38;5;241m&\u001b[39m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlower_bound \u001b[38;5;241m<\u001b[39m\u001b[38;5;241m=\u001b[39m value) \u001b[38;5;241m&\u001b[39m (value \u001b[38;5;241m<\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mupper_bound)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Training\n",
    "model.learn(total_timesteps=600_000, callback=callback)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "ca28e4b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('train/best_model_1700000')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2ca7429",
   "metadata": {},
   "source": [
    "# Continuing Training Many Best Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32f1b38b",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_models = [\n",
    "    \"trial_10_best_model\",\n",
    "    \"trial_11_best_model\",\n",
    "    \"trial_14_best_model\",\n",
    "    \"trial_15_best_model\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3657da17",
   "metadata": {},
   "outputs": [],
   "source": [
    "for model_name in best_models:\n",
    "    # Loading model\n",
    "    model = PPO.load(f\"opt/{model_name}\", env, verbose=1, tensorboard_log=LOG_DIR)\n",
    "    # Lowering learning rate\n",
    "    model.learning_rate /= 3\n",
    "    # Training\n",
    "    model.learn(total_timesteps=2_000_000, callback=callback)\n",
    "    # Saving model\n",
    "    model.save(f'train/old_gen/{model_name}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6c4fa5b",
   "metadata": {},
   "source": [
    "# Creating New Model And Loading Parameters of Diffrent One"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8ea7c5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Best params\n",
    "parameters = {'gamma': 0.9905260007134729, \n",
    "              'n_steps': 6_144,\n",
    "              'learning_rate': 6.0670099142727265e-06, \n",
    "              'clip_range': 0.1865481147395506, \n",
    "              'gae_lambda': 0.9758617427012113\n",
    "             }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4464af8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = PPO('CnnPolicy', env, **parameters, verbose=1, tensorboard_log=LOG_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "760ac269",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.set_parameters(\"train/old_gen/New_Mario_PPO\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6abd7057",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training\n",
    "model.learn(total_timesteps=1_000_000, callback=callback)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67554be0",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save(\"train/Mario_PPO.zip\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
